{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use DeepSurv from the repo\n",
    "import sys\n",
    "sys.path.append('../deepsurv')\n",
    "import deep_surv\n",
    "\n",
    "from deepsurv_logger import DeepSurvLogger, TensorboardLogger\n",
    "import utils\n",
    "import viz\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lasagne\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in dataset\n",
    "First, I read in the dataset and print the first five elements to get a sense of what the dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable_1</th>\n",
       "      <th>Variable_2</th>\n",
       "      <th>Variable_3</th>\n",
       "      <th>Variable_4</th>\n",
       "      <th>Event</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variable_1   Variable_2  Variable_3  Variable_4  Event  Time\n",
       "0            0           3           2         4.6      1    43\n",
       "1            0           2           0         1.6      0    52\n",
       "2            0           3           0         3.5      1    73\n",
       "3            0           3           1         5.1      0    51\n",
       "4            0           2           0         1.7      0    51"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_fp = './Data_for_Jared_True_CSV.txt'\n",
    "train_df = pd.read_csv(dataset_fp)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the dataset to \"DeepSurv\" format\n",
    "DeepSurv expects a dataset to be in the form:\n",
    "\n",
    "    {\n",
    "        'x': numpy array of float32\n",
    "        'e': numpy array of int32\n",
    "        't': numpy array of float32\n",
    "        'hr': (optional) numpy array of float32\n",
    "    }\n",
    "    \n",
    "You are providing me a csv, which I read in as a pandas dataframe. Then I convert the pandas dataframe into the DeepSurv dataset format above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# event_col is the header in the df that represents the 'Event / Status' indicator\n",
    "# time_col is the header in the df that represents the event time\n",
    "def dataframe_to_deepsurv_ds(df, event_col = 'Event', time_col = 'Time'):\n",
    "    # Extract the event and time columns as numpy arrays\n",
    "    e = df[event_col].values.astype(np.int32)\n",
    "    t = df[time_col].values.astype(np.float32)\n",
    "\n",
    "    # Extract the patient's covariates as a numpy array\n",
    "    x_df = df.drop([event_col, time_col], axis = 1)\n",
    "    x = x_df.values.astype(np.float32)\n",
    "    \n",
    "    # Return the deep surv dataframe\n",
    "    return {\n",
    "        'x' : x,\n",
    "        'e' : e,\n",
    "        't' : t\n",
    "    }\n",
    "\n",
    "# If the headers of the csv change, you can replace the values of \n",
    "# 'event_col' and 'time_col' with the names of the new headers\n",
    "# You can also use this function on your training dataset, validation dataset, and testing dataset\n",
    "train_data = dataframe_to_deepsurv_ds(train_df, event_col = 'Event', time_col= 'Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once you have your dataset all formatted, define you hyper_parameters as a Python dictionary. \n",
    "I'll provide you with some example hyper-parameters, but you should replace the values once you tune them to your specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'L2_reg': 10.0,\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.4,\n",
    "    'hidden_layers_sizes': [25, 25],\n",
    "    'learning_rate': 1e-05,\n",
    "    'lr_decay': 0.001,\n",
    "    'momentum': 0.9,\n",
    "    'n_in': train_data['x'].shape[1],\n",
    "    'standardize': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you prepared your dataset, and defined your hyper-parameters. Now it's time to train DeepSurv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing x with\n",
      "2017-06-09 00:44:54,714 - Training step 0/2000    |                         | - loss: 1501.5049 - ci: 0.5311\n",
      "2017-06-09 00:44:54,714 - Training step 0/2000    |                         | - loss: 1501.5049 - ci: 0.5311\n",
      "2017-06-09 00:44:54,714 - Training step 0/2000    |                         | - loss: 1501.5049 - ci: 0.5311\n",
      "2017-06-09 00:44:56,302 - Training step 10/2000   |                         | - loss: 1478.5582 - ci: 0.6236\n",
      "2017-06-09 00:44:56,302 - Training step 10/2000   |                         | - loss: 1478.5582 - ci: 0.6236\n",
      "2017-06-09 00:44:56,302 - Training step 10/2000   |                         | - loss: 1478.5582 - ci: 0.6236\n",
      "2017-06-09 00:44:57,804 - Training step 20/2000   |                         | - loss: 1389.9281 - ci: 0.6942\n",
      "2017-06-09 00:44:57,804 - Training step 20/2000   |                         | - loss: 1389.9281 - ci: 0.6942\n",
      "2017-06-09 00:44:57,804 - Training step 20/2000   |                         | - loss: 1389.9281 - ci: 0.6942\n",
      "2017-06-09 00:44:59,229 - Training step 30/2000   |                         | - loss: 1361.9044 - ci: 0.7034\n",
      "2017-06-09 00:44:59,229 - Training step 30/2000   |                         | - loss: 1361.9044 - ci: 0.7034\n",
      "2017-06-09 00:44:59,229 - Training step 30/2000   |                         | - loss: 1361.9044 - ci: 0.7034\n",
      "2017-06-09 00:45:00,685 - Training step 40/2000   |                         | - loss: 1363.5573 - ci: 0.7058\n",
      "2017-06-09 00:45:00,685 - Training step 40/2000   |                         | - loss: 1363.5573 - ci: 0.7058\n",
      "2017-06-09 00:45:00,685 - Training step 40/2000   |                         | - loss: 1363.5573 - ci: 0.7058\n",
      "2017-06-09 00:45:02,203 - Training step 50/2000   |                         | - loss: 1363.1553 - ci: 0.7111\n",
      "2017-06-09 00:45:02,203 - Training step 50/2000   |                         | - loss: 1363.1553 - ci: 0.7111\n",
      "2017-06-09 00:45:02,203 - Training step 50/2000   |                         | - loss: 1363.1553 - ci: 0.7111\n",
      "2017-06-09 00:45:04,015 - Training step 60/2000   |                         | - loss: 1327.0909 - ci: 0.7124\n",
      "2017-06-09 00:45:04,015 - Training step 60/2000   |                         | - loss: 1327.0909 - ci: 0.7124\n",
      "2017-06-09 00:45:04,015 - Training step 60/2000   |                         | - loss: 1327.0909 - ci: 0.7124\n",
      "2017-06-09 00:45:05,432 - Training step 70/2000   |                         | - loss: 1331.9385 - ci: 0.7112\n",
      "2017-06-09 00:45:05,432 - Training step 70/2000   |                         | - loss: 1331.9385 - ci: 0.7112\n",
      "2017-06-09 00:45:05,432 - Training step 70/2000   |                         | - loss: 1331.9385 - ci: 0.7112\n",
      "2017-06-09 00:45:07,316 - Training step 80/2000   |*                        | - loss: 1331.4475 - ci: 0.7119\n",
      "2017-06-09 00:45:07,316 - Training step 80/2000   |*                        | - loss: 1331.4475 - ci: 0.7119\n",
      "2017-06-09 00:45:07,316 - Training step 80/2000   |*                        | - loss: 1331.4475 - ci: 0.7119\n",
      "2017-06-09 00:45:09,050 - Training step 90/2000   |*                        | - loss: 1324.7543 - ci: 0.7115\n",
      "2017-06-09 00:45:09,050 - Training step 90/2000   |*                        | - loss: 1324.7543 - ci: 0.7115\n",
      "2017-06-09 00:45:09,050 - Training step 90/2000   |*                        | - loss: 1324.7543 - ci: 0.7115\n",
      "2017-06-09 00:45:10,740 - Training step 100/2000  |*                        | - loss: 1328.7374 - ci: 0.7110\n",
      "2017-06-09 00:45:10,740 - Training step 100/2000  |*                        | - loss: 1328.7374 - ci: 0.7110\n",
      "2017-06-09 00:45:10,740 - Training step 100/2000  |*                        | - loss: 1328.7374 - ci: 0.7110\n",
      "2017-06-09 00:45:12,405 - Training step 110/2000  |*                        | - loss: 1325.3510 - ci: 0.7111\n",
      "2017-06-09 00:45:12,405 - Training step 110/2000  |*                        | - loss: 1325.3510 - ci: 0.7111\n",
      "2017-06-09 00:45:12,405 - Training step 110/2000  |*                        | - loss: 1325.3510 - ci: 0.7111\n",
      "2017-06-09 00:45:14,015 - Training step 120/2000  |*                        | - loss: 1319.7078 - ci: 0.7111\n",
      "2017-06-09 00:45:14,015 - Training step 120/2000  |*                        | - loss: 1319.7078 - ci: 0.7111\n",
      "2017-06-09 00:45:14,015 - Training step 120/2000  |*                        | - loss: 1319.7078 - ci: 0.7111\n",
      "2017-06-09 00:45:15,562 - Training step 130/2000  |*                        | - loss: 1324.7845 - ci: 0.7111\n",
      "2017-06-09 00:45:15,562 - Training step 130/2000  |*                        | - loss: 1324.7845 - ci: 0.7111\n",
      "2017-06-09 00:45:15,562 - Training step 130/2000  |*                        | - loss: 1324.7845 - ci: 0.7111\n",
      "2017-06-09 00:45:16,970 - Training step 140/2000  |*                        | - loss: 1309.3800 - ci: 0.7110\n",
      "2017-06-09 00:45:16,970 - Training step 140/2000  |*                        | - loss: 1309.3800 - ci: 0.7110\n",
      "2017-06-09 00:45:16,970 - Training step 140/2000  |*                        | - loss: 1309.3800 - ci: 0.7110\n",
      "2017-06-09 00:45:18,537 - Training step 150/2000  |*                        | - loss: 1318.7681 - ci: 0.7115\n",
      "2017-06-09 00:45:18,537 - Training step 150/2000  |*                        | - loss: 1318.7681 - ci: 0.7115\n",
      "2017-06-09 00:45:18,537 - Training step 150/2000  |*                        | - loss: 1318.7681 - ci: 0.7115\n",
      "2017-06-09 00:45:20,307 - Training step 160/2000  |**                       | - loss: 1312.8696 - ci: 0.7112\n",
      "2017-06-09 00:45:20,307 - Training step 160/2000  |**                       | - loss: 1312.8696 - ci: 0.7112\n",
      "2017-06-09 00:45:20,307 - Training step 160/2000  |**                       | - loss: 1312.8696 - ci: 0.7112\n",
      "2017-06-09 00:45:22,238 - Training step 170/2000  |**                       | - loss: 1314.8079 - ci: 0.7113\n",
      "2017-06-09 00:45:22,238 - Training step 170/2000  |**                       | - loss: 1314.8079 - ci: 0.7113\n",
      "2017-06-09 00:45:22,238 - Training step 170/2000  |**                       | - loss: 1314.8079 - ci: 0.7113\n",
      "2017-06-09 00:45:24,460 - Training step 180/2000  |**                       | - loss: 1308.8923 - ci: 0.7118\n",
      "2017-06-09 00:45:24,460 - Training step 180/2000  |**                       | - loss: 1308.8923 - ci: 0.7118\n",
      "2017-06-09 00:45:24,460 - Training step 180/2000  |**                       | - loss: 1308.8923 - ci: 0.7118\n",
      "2017-06-09 00:45:26,134 - Training step 190/2000  |**                       | - loss: 1318.2069 - ci: 0.7122\n",
      "2017-06-09 00:45:26,134 - Training step 190/2000  |**                       | - loss: 1318.2069 - ci: 0.7122\n",
      "2017-06-09 00:45:26,134 - Training step 190/2000  |**                       | - loss: 1318.2069 - ci: 0.7122\n",
      "2017-06-09 00:45:27,763 - Training step 200/2000  |**                       | - loss: 1315.4109 - ci: 0.7124\n",
      "2017-06-09 00:45:27,763 - Training step 200/2000  |**                       | - loss: 1315.4109 - ci: 0.7124\n",
      "2017-06-09 00:45:27,763 - Training step 200/2000  |**                       | - loss: 1315.4109 - ci: 0.7124\n",
      "2017-06-09 00:45:29,152 - Training step 210/2000  |**                       | - loss: 1320.3564 - ci: 0.7126\n",
      "2017-06-09 00:45:29,152 - Training step 210/2000  |**                       | - loss: 1320.3564 - ci: 0.7126\n",
      "2017-06-09 00:45:29,152 - Training step 210/2000  |**                       | - loss: 1320.3564 - ci: 0.7126\n",
      "2017-06-09 00:45:30,755 - Training step 220/2000  |**                       | - loss: 1321.5094 - ci: 0.7124\n",
      "2017-06-09 00:45:30,755 - Training step 220/2000  |**                       | - loss: 1321.5094 - ci: 0.7124\n",
      "2017-06-09 00:45:30,755 - Training step 220/2000  |**                       | - loss: 1321.5094 - ci: 0.7124\n",
      "2017-06-09 00:45:32,497 - Training step 230/2000  |**                       | - loss: 1316.7141 - ci: 0.7127\n",
      "2017-06-09 00:45:32,497 - Training step 230/2000  |**                       | - loss: 1316.7141 - ci: 0.7127\n",
      "2017-06-09 00:45:32,497 - Training step 230/2000  |**                       | - loss: 1316.7141 - ci: 0.7127\n",
      "2017-06-09 00:45:33,980 - Training step 240/2000  |***                      | - loss: 1308.3854 - ci: 0.7117\n",
      "2017-06-09 00:45:33,980 - Training step 240/2000  |***                      | - loss: 1308.3854 - ci: 0.7117\n",
      "2017-06-09 00:45:33,980 - Training step 240/2000  |***                      | - loss: 1308.3854 - ci: 0.7117\n",
      "2017-06-09 00:45:35,628 - Training step 250/2000  |***                      | - loss: 1327.4872 - ci: 0.7125\n",
      "2017-06-09 00:45:35,628 - Training step 250/2000  |***                      | - loss: 1327.4872 - ci: 0.7125\n",
      "2017-06-09 00:45:35,628 - Training step 250/2000  |***                      | - loss: 1327.4872 - ci: 0.7125\n",
      "2017-06-09 00:45:37,138 - Training step 260/2000  |***                      | - loss: 1302.3836 - ci: 0.7128\n",
      "2017-06-09 00:45:37,138 - Training step 260/2000  |***                      | - loss: 1302.3836 - ci: 0.7128\n",
      "2017-06-09 00:45:37,138 - Training step 260/2000  |***                      | - loss: 1302.3836 - ci: 0.7128\n",
      "2017-06-09 00:45:38,510 - Training step 270/2000  |***                      | - loss: 1317.4918 - ci: 0.7130\n",
      "2017-06-09 00:45:38,510 - Training step 270/2000  |***                      | - loss: 1317.4918 - ci: 0.7130\n",
      "2017-06-09 00:45:38,510 - Training step 270/2000  |***                      | - loss: 1317.4918 - ci: 0.7130\n",
      "2017-06-09 00:45:40,011 - Training step 280/2000  |***                      | - loss: 1309.9626 - ci: 0.7128\n",
      "2017-06-09 00:45:40,011 - Training step 280/2000  |***                      | - loss: 1309.9626 - ci: 0.7128\n",
      "2017-06-09 00:45:40,011 - Training step 280/2000  |***                      | - loss: 1309.9626 - ci: 0.7128\n",
      "2017-06-09 00:45:41,524 - Training step 290/2000  |***                      | - loss: 1317.1891 - ci: 0.7131\n",
      "2017-06-09 00:45:41,524 - Training step 290/2000  |***                      | - loss: 1317.1891 - ci: 0.7131\n",
      "2017-06-09 00:45:41,524 - Training step 290/2000  |***                      | - loss: 1317.1891 - ci: 0.7131\n",
      "2017-06-09 00:45:42,907 - Training step 300/2000  |***                      | - loss: 1315.3977 - ci: 0.7133\n",
      "2017-06-09 00:45:42,907 - Training step 300/2000  |***                      | - loss: 1315.3977 - ci: 0.7133\n",
      "2017-06-09 00:45:42,907 - Training step 300/2000  |***                      | - loss: 1315.3977 - ci: 0.7133\n",
      "2017-06-09 00:45:44,429 - Training step 310/2000  |***                      | - loss: 1316.5371 - ci: 0.7129\n",
      "2017-06-09 00:45:44,429 - Training step 310/2000  |***                      | - loss: 1316.5371 - ci: 0.7129\n",
      "2017-06-09 00:45:44,429 - Training step 310/2000  |***                      | - loss: 1316.5371 - ci: 0.7129\n",
      "2017-06-09 00:45:46,005 - Training step 320/2000  |****                     | - loss: 1311.9975 - ci: 0.7136\n",
      "2017-06-09 00:45:46,005 - Training step 320/2000  |****                     | - loss: 1311.9975 - ci: 0.7136\n",
      "2017-06-09 00:45:46,005 - Training step 320/2000  |****                     | - loss: 1311.9975 - ci: 0.7136\n",
      "2017-06-09 00:45:47,560 - Training step 330/2000  |****                     | - loss: 1319.2411 - ci: 0.7133\n",
      "2017-06-09 00:45:47,560 - Training step 330/2000  |****                     | - loss: 1319.2411 - ci: 0.7133\n",
      "2017-06-09 00:45:47,560 - Training step 330/2000  |****                     | - loss: 1319.2411 - ci: 0.7133\n",
      "2017-06-09 00:45:48,938 - Training step 340/2000  |****                     | - loss: 1314.4581 - ci: 0.7136\n",
      "2017-06-09 00:45:48,938 - Training step 340/2000  |****                     | - loss: 1314.4581 - ci: 0.7136\n",
      "2017-06-09 00:45:48,938 - Training step 340/2000  |****                     | - loss: 1314.4581 - ci: 0.7136\n",
      "2017-06-09 00:45:50,499 - Training step 350/2000  |****                     | - loss: 1300.1730 - ci: 0.7133\n",
      "2017-06-09 00:45:50,499 - Training step 350/2000  |****                     | - loss: 1300.1730 - ci: 0.7133\n",
      "2017-06-09 00:45:50,499 - Training step 350/2000  |****                     | - loss: 1300.1730 - ci: 0.7133\n",
      "2017-06-09 00:45:52,190 - Training step 360/2000  |****                     | - loss: 1312.1615 - ci: 0.7137\n",
      "2017-06-09 00:45:52,190 - Training step 360/2000  |****                     | - loss: 1312.1615 - ci: 0.7137\n",
      "2017-06-09 00:45:52,190 - Training step 360/2000  |****                     | - loss: 1312.1615 - ci: 0.7137\n",
      "2017-06-09 00:45:53,702 - Training step 370/2000  |****                     | - loss: 1313.1401 - ci: 0.7138\n",
      "2017-06-09 00:45:53,702 - Training step 370/2000  |****                     | - loss: 1313.1401 - ci: 0.7138\n",
      "2017-06-09 00:45:53,702 - Training step 370/2000  |****                     | - loss: 1313.1401 - ci: 0.7138\n",
      "2017-06-09 00:45:55,376 - Training step 380/2000  |****                     | - loss: 1299.6676 - ci: 0.7139\n",
      "2017-06-09 00:45:55,376 - Training step 380/2000  |****                     | - loss: 1299.6676 - ci: 0.7139\n",
      "2017-06-09 00:45:55,376 - Training step 380/2000  |****                     | - loss: 1299.6676 - ci: 0.7139\n",
      "2017-06-09 00:45:56,958 - Training step 390/2000  |****                     | - loss: 1308.9985 - ci: 0.7132\n",
      "2017-06-09 00:45:56,958 - Training step 390/2000  |****                     | - loss: 1308.9985 - ci: 0.7132\n",
      "2017-06-09 00:45:56,958 - Training step 390/2000  |****                     | - loss: 1308.9985 - ci: 0.7132\n",
      "2017-06-09 00:45:58,573 - Training step 400/2000  |*****                    | - loss: 1314.8281 - ci: 0.7137\n",
      "2017-06-09 00:45:58,573 - Training step 400/2000  |*****                    | - loss: 1314.8281 - ci: 0.7137\n",
      "2017-06-09 00:45:58,573 - Training step 400/2000  |*****                    | - loss: 1314.8281 - ci: 0.7137\n",
      "2017-06-09 00:46:00,012 - Training step 410/2000  |*****                    | - loss: 1309.8234 - ci: 0.7143\n",
      "2017-06-09 00:46:00,012 - Training step 410/2000  |*****                    | - loss: 1309.8234 - ci: 0.7143\n",
      "2017-06-09 00:46:00,012 - Training step 410/2000  |*****                    | - loss: 1309.8234 - ci: 0.7143\n",
      "2017-06-09 00:46:01,531 - Training step 420/2000  |*****                    | - loss: 1310.8387 - ci: 0.7139\n",
      "2017-06-09 00:46:01,531 - Training step 420/2000  |*****                    | - loss: 1310.8387 - ci: 0.7139\n",
      "2017-06-09 00:46:01,531 - Training step 420/2000  |*****                    | - loss: 1310.8387 - ci: 0.7139\n",
      "2017-06-09 00:46:03,041 - Training step 430/2000  |*****                    | - loss: 1307.7185 - ci: 0.7143\n",
      "2017-06-09 00:46:03,041 - Training step 430/2000  |*****                    | - loss: 1307.7185 - ci: 0.7143\n",
      "2017-06-09 00:46:03,041 - Training step 430/2000  |*****                    | - loss: 1307.7185 - ci: 0.7143\n",
      "2017-06-09 00:46:04,419 - Training step 440/2000  |*****                    | - loss: 1315.8556 - ci: 0.7141\n",
      "2017-06-09 00:46:04,419 - Training step 440/2000  |*****                    | - loss: 1315.8556 - ci: 0.7141\n",
      "2017-06-09 00:46:04,419 - Training step 440/2000  |*****                    | - loss: 1315.8556 - ci: 0.7141\n",
      "2017-06-09 00:46:05,925 - Training step 450/2000  |*****                    | - loss: 1318.6137 - ci: 0.7144\n",
      "2017-06-09 00:46:05,925 - Training step 450/2000  |*****                    | - loss: 1318.6137 - ci: 0.7144\n",
      "2017-06-09 00:46:05,925 - Training step 450/2000  |*****                    | - loss: 1318.6137 - ci: 0.7144\n",
      "2017-06-09 00:46:07,380 - Training step 460/2000  |*****                    | - loss: 1316.2490 - ci: 0.7148\n",
      "2017-06-09 00:46:07,380 - Training step 460/2000  |*****                    | - loss: 1316.2490 - ci: 0.7148\n",
      "2017-06-09 00:46:07,380 - Training step 460/2000  |*****                    | - loss: 1316.2490 - ci: 0.7148\n",
      "2017-06-09 00:46:08,822 - Training step 470/2000  |*****                    | - loss: 1311.4232 - ci: 0.7144\n",
      "2017-06-09 00:46:08,822 - Training step 470/2000  |*****                    | - loss: 1311.4232 - ci: 0.7144\n",
      "2017-06-09 00:46:08,822 - Training step 470/2000  |*****                    | - loss: 1311.4232 - ci: 0.7144\n",
      "2017-06-09 00:46:10,284 - Training step 480/2000  |******                   | - loss: 1317.0009 - ci: 0.7148\n",
      "2017-06-09 00:46:10,284 - Training step 480/2000  |******                   | - loss: 1317.0009 - ci: 0.7148\n",
      "2017-06-09 00:46:10,284 - Training step 480/2000  |******                   | - loss: 1317.0009 - ci: 0.7148\n",
      "2017-06-09 00:46:11,874 - Training step 490/2000  |******                   | - loss: 1310.4235 - ci: 0.7146\n",
      "2017-06-09 00:46:11,874 - Training step 490/2000  |******                   | - loss: 1310.4235 - ci: 0.7146\n",
      "2017-06-09 00:46:11,874 - Training step 490/2000  |******                   | - loss: 1310.4235 - ci: 0.7146\n",
      "2017-06-09 00:46:13,218 - Training step 500/2000  |******                   | - loss: 1303.0604 - ci: 0.7145\n",
      "2017-06-09 00:46:13,218 - Training step 500/2000  |******                   | - loss: 1303.0604 - ci: 0.7145\n",
      "2017-06-09 00:46:13,218 - Training step 500/2000  |******                   | - loss: 1303.0604 - ci: 0.7145\n",
      "2017-06-09 00:46:14,763 - Training step 510/2000  |******                   | - loss: 1309.1406 - ci: 0.7146\n",
      "2017-06-09 00:46:14,763 - Training step 510/2000  |******                   | - loss: 1309.1406 - ci: 0.7146\n",
      "2017-06-09 00:46:14,763 - Training step 510/2000  |******                   | - loss: 1309.1406 - ci: 0.7146\n",
      "2017-06-09 00:46:16,269 - Training step 520/2000  |******                   | - loss: 1305.7442 - ci: 0.7147\n",
      "2017-06-09 00:46:16,269 - Training step 520/2000  |******                   | - loss: 1305.7442 - ci: 0.7147\n",
      "2017-06-09 00:46:16,269 - Training step 520/2000  |******                   | - loss: 1305.7442 - ci: 0.7147\n",
      "2017-06-09 00:46:17,861 - Training step 530/2000  |******                   | - loss: 1321.5040 - ci: 0.7148\n",
      "2017-06-09 00:46:17,861 - Training step 530/2000  |******                   | - loss: 1321.5040 - ci: 0.7148\n",
      "2017-06-09 00:46:17,861 - Training step 530/2000  |******                   | - loss: 1321.5040 - ci: 0.7148\n",
      "2017-06-09 00:46:19,286 - Training step 540/2000  |******                   | - loss: 1308.5344 - ci: 0.7151\n",
      "2017-06-09 00:46:19,286 - Training step 540/2000  |******                   | - loss: 1308.5344 - ci: 0.7151\n",
      "2017-06-09 00:46:19,286 - Training step 540/2000  |******                   | - loss: 1308.5344 - ci: 0.7151\n",
      "2017-06-09 00:46:20,777 - Training step 550/2000  |******                   | - loss: 1310.4202 - ci: 0.7147\n",
      "2017-06-09 00:46:20,777 - Training step 550/2000  |******                   | - loss: 1310.4202 - ci: 0.7147\n",
      "2017-06-09 00:46:20,777 - Training step 550/2000  |******                   | - loss: 1310.4202 - ci: 0.7147\n",
      "2017-06-09 00:46:22,408 - Training step 560/2000  |*******                  | - loss: 1309.8176 - ci: 0.7141\n",
      "2017-06-09 00:46:22,408 - Training step 560/2000  |*******                  | - loss: 1309.8176 - ci: 0.7141\n",
      "2017-06-09 00:46:22,408 - Training step 560/2000  |*******                  | - loss: 1309.8176 - ci: 0.7141\n",
      "2017-06-09 00:46:23,754 - Training step 570/2000  |*******                  | - loss: 1303.7040 - ci: 0.7143\n",
      "2017-06-09 00:46:23,754 - Training step 570/2000  |*******                  | - loss: 1303.7040 - ci: 0.7143\n",
      "2017-06-09 00:46:23,754 - Training step 570/2000  |*******                  | - loss: 1303.7040 - ci: 0.7143\n",
      "2017-06-09 00:46:25,269 - Training step 580/2000  |*******                  | - loss: 1308.3771 - ci: 0.7145\n",
      "2017-06-09 00:46:25,269 - Training step 580/2000  |*******                  | - loss: 1308.3771 - ci: 0.7145\n",
      "2017-06-09 00:46:25,269 - Training step 580/2000  |*******                  | - loss: 1308.3771 - ci: 0.7145\n",
      "2017-06-09 00:46:26,854 - Training step 590/2000  |*******                  | - loss: 1309.0045 - ci: 0.7148\n",
      "2017-06-09 00:46:26,854 - Training step 590/2000  |*******                  | - loss: 1309.0045 - ci: 0.7148\n",
      "2017-06-09 00:46:26,854 - Training step 590/2000  |*******                  | - loss: 1309.0045 - ci: 0.7148\n",
      "2017-06-09 00:46:28,362 - Training step 600/2000  |*******                  | - loss: 1321.5077 - ci: 0.7150\n",
      "2017-06-09 00:46:28,362 - Training step 600/2000  |*******                  | - loss: 1321.5077 - ci: 0.7150\n",
      "2017-06-09 00:46:28,362 - Training step 600/2000  |*******                  | - loss: 1321.5077 - ci: 0.7150\n",
      "2017-06-09 00:46:29,759 - Training step 610/2000  |*******                  | - loss: 1309.3300 - ci: 0.7149\n",
      "2017-06-09 00:46:29,759 - Training step 610/2000  |*******                  | - loss: 1309.3300 - ci: 0.7149\n",
      "2017-06-09 00:46:29,759 - Training step 610/2000  |*******                  | - loss: 1309.3300 - ci: 0.7149\n",
      "2017-06-09 00:46:31,296 - Training step 620/2000  |*******                  | - loss: 1310.1265 - ci: 0.7150\n",
      "2017-06-09 00:46:31,296 - Training step 620/2000  |*******                  | - loss: 1310.1265 - ci: 0.7150\n",
      "2017-06-09 00:46:31,296 - Training step 620/2000  |*******                  | - loss: 1310.1265 - ci: 0.7150\n",
      "2017-06-09 00:46:32,829 - Training step 630/2000  |*******                  | - loss: 1312.2480 - ci: 0.7150\n",
      "2017-06-09 00:46:32,829 - Training step 630/2000  |*******                  | - loss: 1312.2480 - ci: 0.7150\n",
      "2017-06-09 00:46:32,829 - Training step 630/2000  |*******                  | - loss: 1312.2480 - ci: 0.7150\n",
      "2017-06-09 00:46:34,199 - Training step 640/2000  |********                 | - loss: 1305.7918 - ci: 0.7151\n",
      "2017-06-09 00:46:34,199 - Training step 640/2000  |********                 | - loss: 1305.7918 - ci: 0.7151\n",
      "2017-06-09 00:46:34,199 - Training step 640/2000  |********                 | - loss: 1305.7918 - ci: 0.7151\n",
      "2017-06-09 00:46:35,686 - Training step 650/2000  |********                 | - loss: 1307.9812 - ci: 0.7151\n",
      "2017-06-09 00:46:35,686 - Training step 650/2000  |********                 | - loss: 1307.9812 - ci: 0.7151\n",
      "2017-06-09 00:46:35,686 - Training step 650/2000  |********                 | - loss: 1307.9812 - ci: 0.7151\n",
      "2017-06-09 00:46:37,171 - Training step 660/2000  |********                 | - loss: 1316.7786 - ci: 0.7145\n",
      "2017-06-09 00:46:37,171 - Training step 660/2000  |********                 | - loss: 1316.7786 - ci: 0.7145\n",
      "2017-06-09 00:46:37,171 - Training step 660/2000  |********                 | - loss: 1316.7786 - ci: 0.7145\n",
      "2017-06-09 00:46:38,746 - Training step 670/2000  |********                 | - loss: 1303.2462 - ci: 0.7143\n",
      "2017-06-09 00:46:38,746 - Training step 670/2000  |********                 | - loss: 1303.2462 - ci: 0.7143\n",
      "2017-06-09 00:46:38,746 - Training step 670/2000  |********                 | - loss: 1303.2462 - ci: 0.7143\n",
      "2017-06-09 00:46:40,349 - Training step 680/2000  |********                 | - loss: 1306.9628 - ci: 0.7148\n",
      "2017-06-09 00:46:40,349 - Training step 680/2000  |********                 | - loss: 1306.9628 - ci: 0.7148\n",
      "2017-06-09 00:46:40,349 - Training step 680/2000  |********                 | - loss: 1306.9628 - ci: 0.7148\n",
      "2017-06-09 00:46:41,841 - Training step 690/2000  |********                 | - loss: 1311.0211 - ci: 0.7154\n",
      "2017-06-09 00:46:41,841 - Training step 690/2000  |********                 | - loss: 1311.0211 - ci: 0.7154\n",
      "2017-06-09 00:46:41,841 - Training step 690/2000  |********                 | - loss: 1311.0211 - ci: 0.7154\n",
      "2017-06-09 00:46:43,442 - Training step 700/2000  |********                 | - loss: 1312.8577 - ci: 0.7156\n",
      "2017-06-09 00:46:43,442 - Training step 700/2000  |********                 | - loss: 1312.8577 - ci: 0.7156\n",
      "2017-06-09 00:46:43,442 - Training step 700/2000  |********                 | - loss: 1312.8577 - ci: 0.7156\n",
      "2017-06-09 00:46:44,795 - Training step 710/2000  |********                 | - loss: 1308.3466 - ci: 0.7151\n",
      "2017-06-09 00:46:44,795 - Training step 710/2000  |********                 | - loss: 1308.3466 - ci: 0.7151\n",
      "2017-06-09 00:46:44,795 - Training step 710/2000  |********                 | - loss: 1308.3466 - ci: 0.7151\n",
      "2017-06-09 00:46:46,242 - Training step 720/2000  |*********                | - loss: 1314.3357 - ci: 0.7151\n",
      "2017-06-09 00:46:46,242 - Training step 720/2000  |*********                | - loss: 1314.3357 - ci: 0.7151\n",
      "2017-06-09 00:46:46,242 - Training step 720/2000  |*********                | - loss: 1314.3357 - ci: 0.7151\n",
      "2017-06-09 00:46:47,763 - Training step 730/2000  |*********                | - loss: 1302.6228 - ci: 0.7154\n",
      "2017-06-09 00:46:47,763 - Training step 730/2000  |*********                | - loss: 1302.6228 - ci: 0.7154\n",
      "2017-06-09 00:46:47,763 - Training step 730/2000  |*********                | - loss: 1302.6228 - ci: 0.7154\n",
      "2017-06-09 00:46:49,238 - Training step 740/2000  |*********                | - loss: 1319.1355 - ci: 0.7155\n",
      "2017-06-09 00:46:49,238 - Training step 740/2000  |*********                | - loss: 1319.1355 - ci: 0.7155\n",
      "2017-06-09 00:46:49,238 - Training step 740/2000  |*********                | - loss: 1319.1355 - ci: 0.7155\n",
      "2017-06-09 00:46:50,971 - Training step 750/2000  |*********                | - loss: 1312.0689 - ci: 0.7157\n",
      "2017-06-09 00:46:50,971 - Training step 750/2000  |*********                | - loss: 1312.0689 - ci: 0.7157\n",
      "2017-06-09 00:46:50,971 - Training step 750/2000  |*********                | - loss: 1312.0689 - ci: 0.7157\n",
      "2017-06-09 00:46:52,586 - Training step 760/2000  |*********                | - loss: 1311.4020 - ci: 0.7157\n",
      "2017-06-09 00:46:52,586 - Training step 760/2000  |*********                | - loss: 1311.4020 - ci: 0.7157\n",
      "2017-06-09 00:46:52,586 - Training step 760/2000  |*********                | - loss: 1311.4020 - ci: 0.7157\n",
      "2017-06-09 00:46:54,225 - Training step 770/2000  |*********                | - loss: 1301.0336 - ci: 0.7156\n",
      "2017-06-09 00:46:54,225 - Training step 770/2000  |*********                | - loss: 1301.0336 - ci: 0.7156\n",
      "2017-06-09 00:46:54,225 - Training step 770/2000  |*********                | - loss: 1301.0336 - ci: 0.7156\n",
      "2017-06-09 00:46:55,608 - Training step 780/2000  |*********                | - loss: 1311.9334 - ci: 0.7157\n",
      "2017-06-09 00:46:55,608 - Training step 780/2000  |*********                | - loss: 1311.9334 - ci: 0.7157\n",
      "2017-06-09 00:46:55,608 - Training step 780/2000  |*********                | - loss: 1311.9334 - ci: 0.7157\n",
      "2017-06-09 00:46:57,072 - Training step 790/2000  |*********                | - loss: 1305.5237 - ci: 0.7157\n",
      "2017-06-09 00:46:57,072 - Training step 790/2000  |*********                | - loss: 1305.5237 - ci: 0.7157\n",
      "2017-06-09 00:46:57,072 - Training step 790/2000  |*********                | - loss: 1305.5237 - ci: 0.7157\n",
      "2017-06-09 00:46:58,534 - Training step 800/2000  |**********               | - loss: 1305.0344 - ci: 0.7156\n",
      "2017-06-09 00:46:58,534 - Training step 800/2000  |**********               | - loss: 1305.0344 - ci: 0.7156\n",
      "2017-06-09 00:46:58,534 - Training step 800/2000  |**********               | - loss: 1305.0344 - ci: 0.7156\n",
      "2017-06-09 00:46:59,910 - Training step 810/2000  |**********               | - loss: 1309.2998 - ci: 0.7157\n",
      "2017-06-09 00:46:59,910 - Training step 810/2000  |**********               | - loss: 1309.2998 - ci: 0.7157\n",
      "2017-06-09 00:46:59,910 - Training step 810/2000  |**********               | - loss: 1309.2998 - ci: 0.7157\n",
      "2017-06-09 00:47:01,685 - Training step 820/2000  |**********               | - loss: 1307.1702 - ci: 0.7156\n",
      "2017-06-09 00:47:01,685 - Training step 820/2000  |**********               | - loss: 1307.1702 - ci: 0.7156\n",
      "2017-06-09 00:47:01,685 - Training step 820/2000  |**********               | - loss: 1307.1702 - ci: 0.7156\n",
      "2017-06-09 00:47:03,399 - Training step 830/2000  |**********               | - loss: 1312.7261 - ci: 0.7158\n",
      "2017-06-09 00:47:03,399 - Training step 830/2000  |**********               | - loss: 1312.7261 - ci: 0.7158\n",
      "2017-06-09 00:47:03,399 - Training step 830/2000  |**********               | - loss: 1312.7261 - ci: 0.7158\n",
      "2017-06-09 00:47:04,907 - Training step 840/2000  |**********               | - loss: 1311.6965 - ci: 0.7155\n",
      "2017-06-09 00:47:04,907 - Training step 840/2000  |**********               | - loss: 1311.6965 - ci: 0.7155\n",
      "2017-06-09 00:47:04,907 - Training step 840/2000  |**********               | - loss: 1311.6965 - ci: 0.7155\n",
      "2017-06-09 00:47:06,431 - Training step 850/2000  |**********               | - loss: 1317.9277 - ci: 0.7156\n",
      "2017-06-09 00:47:06,431 - Training step 850/2000  |**********               | - loss: 1317.9277 - ci: 0.7156\n",
      "2017-06-09 00:47:06,431 - Training step 850/2000  |**********               | - loss: 1317.9277 - ci: 0.7156\n",
      "2017-06-09 00:47:08,040 - Training step 860/2000  |**********               | - loss: 1303.9091 - ci: 0.7159\n",
      "2017-06-09 00:47:08,040 - Training step 860/2000  |**********               | - loss: 1303.9091 - ci: 0.7159\n",
      "2017-06-09 00:47:08,040 - Training step 860/2000  |**********               | - loss: 1303.9091 - ci: 0.7159\n",
      "2017-06-09 00:47:09,499 - Training step 870/2000  |**********               | - loss: 1304.9647 - ci: 0.7162\n",
      "2017-06-09 00:47:09,499 - Training step 870/2000  |**********               | - loss: 1304.9647 - ci: 0.7162\n",
      "2017-06-09 00:47:09,499 - Training step 870/2000  |**********               | - loss: 1304.9647 - ci: 0.7162\n",
      "2017-06-09 00:47:11,105 - Training step 880/2000  |***********              | - loss: 1313.2321 - ci: 0.7158\n",
      "2017-06-09 00:47:11,105 - Training step 880/2000  |***********              | - loss: 1313.2321 - ci: 0.7158\n",
      "2017-06-09 00:47:11,105 - Training step 880/2000  |***********              | - loss: 1313.2321 - ci: 0.7158\n",
      "2017-06-09 00:47:12,664 - Training step 890/2000  |***********              | - loss: 1308.1376 - ci: 0.7160\n",
      "2017-06-09 00:47:12,664 - Training step 890/2000  |***********              | - loss: 1308.1376 - ci: 0.7160\n",
      "2017-06-09 00:47:12,664 - Training step 890/2000  |***********              | - loss: 1308.1376 - ci: 0.7160\n",
      "2017-06-09 00:47:14,171 - Training step 900/2000  |***********              | - loss: 1301.8409 - ci: 0.7162\n",
      "2017-06-09 00:47:14,171 - Training step 900/2000  |***********              | - loss: 1301.8409 - ci: 0.7162\n",
      "2017-06-09 00:47:14,171 - Training step 900/2000  |***********              | - loss: 1301.8409 - ci: 0.7162\n",
      "2017-06-09 00:47:15,781 - Training step 910/2000  |***********              | - loss: 1312.9708 - ci: 0.7160\n",
      "2017-06-09 00:47:15,781 - Training step 910/2000  |***********              | - loss: 1312.9708 - ci: 0.7160\n",
      "2017-06-09 00:47:15,781 - Training step 910/2000  |***********              | - loss: 1312.9708 - ci: 0.7160\n",
      "2017-06-09 00:47:17,316 - Training step 920/2000  |***********              | - loss: 1310.2185 - ci: 0.7159\n",
      "2017-06-09 00:47:17,316 - Training step 920/2000  |***********              | - loss: 1310.2185 - ci: 0.7159\n",
      "2017-06-09 00:47:17,316 - Training step 920/2000  |***********              | - loss: 1310.2185 - ci: 0.7159\n",
      "2017-06-09 00:47:18,840 - Training step 930/2000  |***********              | - loss: 1307.2955 - ci: 0.7157\n",
      "2017-06-09 00:47:18,840 - Training step 930/2000  |***********              | - loss: 1307.2955 - ci: 0.7157\n",
      "2017-06-09 00:47:18,840 - Training step 930/2000  |***********              | - loss: 1307.2955 - ci: 0.7157\n",
      "2017-06-09 00:47:20,337 - Training step 940/2000  |***********              | - loss: 1313.8765 - ci: 0.7161\n",
      "2017-06-09 00:47:20,337 - Training step 940/2000  |***********              | - loss: 1313.8765 - ci: 0.7161\n",
      "2017-06-09 00:47:20,337 - Training step 940/2000  |***********              | - loss: 1313.8765 - ci: 0.7161\n",
      "2017-06-09 00:47:21,886 - Training step 950/2000  |***********              | - loss: 1301.6431 - ci: 0.7162\n",
      "2017-06-09 00:47:21,886 - Training step 950/2000  |***********              | - loss: 1301.6431 - ci: 0.7162\n",
      "2017-06-09 00:47:21,886 - Training step 950/2000  |***********              | - loss: 1301.6431 - ci: 0.7162\n",
      "2017-06-09 00:47:23,650 - Training step 960/2000  |************             | - loss: 1314.4310 - ci: 0.7162\n",
      "2017-06-09 00:47:23,650 - Training step 960/2000  |************             | - loss: 1314.4310 - ci: 0.7162\n",
      "2017-06-09 00:47:23,650 - Training step 960/2000  |************             | - loss: 1314.4310 - ci: 0.7162\n",
      "2017-06-09 00:47:25,103 - Training step 970/2000  |************             | - loss: 1309.2297 - ci: 0.7163\n",
      "2017-06-09 00:47:25,103 - Training step 970/2000  |************             | - loss: 1309.2297 - ci: 0.7163\n",
      "2017-06-09 00:47:25,103 - Training step 970/2000  |************             | - loss: 1309.2297 - ci: 0.7163\n",
      "2017-06-09 00:47:26,558 - Training step 980/2000  |************             | - loss: 1307.6507 - ci: 0.7162\n",
      "2017-06-09 00:47:26,558 - Training step 980/2000  |************             | - loss: 1307.6507 - ci: 0.7162\n",
      "2017-06-09 00:47:26,558 - Training step 980/2000  |************             | - loss: 1307.6507 - ci: 0.7162\n",
      "2017-06-09 00:47:28,080 - Training step 990/2000  |************             | - loss: 1309.8273 - ci: 0.7161\n",
      "2017-06-09 00:47:28,080 - Training step 990/2000  |************             | - loss: 1309.8273 - ci: 0.7161\n",
      "2017-06-09 00:47:28,080 - Training step 990/2000  |************             | - loss: 1309.8273 - ci: 0.7161\n",
      "2017-06-09 00:47:29,656 - Training step 1000/2000 |************             | - loss: 1305.8391 - ci: 0.7164\n",
      "2017-06-09 00:47:29,656 - Training step 1000/2000 |************             | - loss: 1305.8391 - ci: 0.7164\n",
      "2017-06-09 00:47:29,656 - Training step 1000/2000 |************             | - loss: 1305.8391 - ci: 0.7164\n",
      "2017-06-09 00:47:31,035 - Training step 1010/2000 |************             | - loss: 1302.2721 - ci: 0.7163\n",
      "2017-06-09 00:47:31,035 - Training step 1010/2000 |************             | - loss: 1302.2721 - ci: 0.7163\n",
      "2017-06-09 00:47:31,035 - Training step 1010/2000 |************             | - loss: 1302.2721 - ci: 0.7163\n",
      "2017-06-09 00:47:32,559 - Training step 1020/2000 |************             | - loss: 1323.5172 - ci: 0.7166\n",
      "2017-06-09 00:47:32,559 - Training step 1020/2000 |************             | - loss: 1323.5172 - ci: 0.7166\n",
      "2017-06-09 00:47:32,559 - Training step 1020/2000 |************             | - loss: 1323.5172 - ci: 0.7166\n",
      "2017-06-09 00:47:34,097 - Training step 1030/2000 |************             | - loss: 1306.0175 - ci: 0.7162\n",
      "2017-06-09 00:47:34,097 - Training step 1030/2000 |************             | - loss: 1306.0175 - ci: 0.7162\n",
      "2017-06-09 00:47:34,097 - Training step 1030/2000 |************             | - loss: 1306.0175 - ci: 0.7162\n",
      "2017-06-09 00:47:35,568 - Training step 1040/2000 |*************            | - loss: 1308.6968 - ci: 0.7166\n",
      "2017-06-09 00:47:35,568 - Training step 1040/2000 |*************            | - loss: 1308.6968 - ci: 0.7166\n",
      "2017-06-09 00:47:35,568 - Training step 1040/2000 |*************            | - loss: 1308.6968 - ci: 0.7166\n",
      "2017-06-09 00:47:37,153 - Training step 1050/2000 |*************            | - loss: 1306.8967 - ci: 0.7158\n",
      "2017-06-09 00:47:37,153 - Training step 1050/2000 |*************            | - loss: 1306.8967 - ci: 0.7158\n",
      "2017-06-09 00:47:37,153 - Training step 1050/2000 |*************            | - loss: 1306.8967 - ci: 0.7158\n",
      "2017-06-09 00:47:38,800 - Training step 1060/2000 |*************            | - loss: 1314.9792 - ci: 0.7164\n",
      "2017-06-09 00:47:38,800 - Training step 1060/2000 |*************            | - loss: 1314.9792 - ci: 0.7164\n",
      "2017-06-09 00:47:38,800 - Training step 1060/2000 |*************            | - loss: 1314.9792 - ci: 0.7164\n",
      "2017-06-09 00:47:40,202 - Training step 1070/2000 |*************            | - loss: 1303.2147 - ci: 0.7166\n",
      "2017-06-09 00:47:40,202 - Training step 1070/2000 |*************            | - loss: 1303.2147 - ci: 0.7166\n",
      "2017-06-09 00:47:40,202 - Training step 1070/2000 |*************            | - loss: 1303.2147 - ci: 0.7166\n",
      "2017-06-09 00:47:41,719 - Training step 1080/2000 |*************            | - loss: 1303.9220 - ci: 0.7156\n",
      "2017-06-09 00:47:41,719 - Training step 1080/2000 |*************            | - loss: 1303.9220 - ci: 0.7156\n",
      "2017-06-09 00:47:41,719 - Training step 1080/2000 |*************            | - loss: 1303.9220 - ci: 0.7156\n",
      "2017-06-09 00:47:43,347 - Training step 1090/2000 |*************            | - loss: 1307.8010 - ci: 0.7166\n",
      "2017-06-09 00:47:43,347 - Training step 1090/2000 |*************            | - loss: 1307.8010 - ci: 0.7166\n",
      "2017-06-09 00:47:43,347 - Training step 1090/2000 |*************            | - loss: 1307.8010 - ci: 0.7166\n",
      "2017-06-09 00:47:44,743 - Training step 1100/2000 |*************            | - loss: 1316.5170 - ci: 0.7163\n",
      "2017-06-09 00:47:44,743 - Training step 1100/2000 |*************            | - loss: 1316.5170 - ci: 0.7163\n",
      "2017-06-09 00:47:44,743 - Training step 1100/2000 |*************            | - loss: 1316.5170 - ci: 0.7163\n",
      "2017-06-09 00:47:46,306 - Training step 1110/2000 |*************            | - loss: 1306.5222 - ci: 0.7165\n",
      "2017-06-09 00:47:46,306 - Training step 1110/2000 |*************            | - loss: 1306.5222 - ci: 0.7165\n",
      "2017-06-09 00:47:46,306 - Training step 1110/2000 |*************            | - loss: 1306.5222 - ci: 0.7165\n",
      "2017-06-09 00:47:47,805 - Training step 1120/2000 |**************           | - loss: 1305.7558 - ci: 0.7163\n",
      "2017-06-09 00:47:47,805 - Training step 1120/2000 |**************           | - loss: 1305.7558 - ci: 0.7163\n",
      "2017-06-09 00:47:47,805 - Training step 1120/2000 |**************           | - loss: 1305.7558 - ci: 0.7163\n",
      "2017-06-09 00:47:49,293 - Training step 1130/2000 |**************           | - loss: 1306.3765 - ci: 0.7166\n",
      "2017-06-09 00:47:49,293 - Training step 1130/2000 |**************           | - loss: 1306.3765 - ci: 0.7166\n",
      "2017-06-09 00:47:49,293 - Training step 1130/2000 |**************           | - loss: 1306.3765 - ci: 0.7166\n",
      "2017-06-09 00:47:50,771 - Training step 1140/2000 |**************           | - loss: 1313.2537 - ci: 0.7170\n",
      "2017-06-09 00:47:50,771 - Training step 1140/2000 |**************           | - loss: 1313.2537 - ci: 0.7170\n",
      "2017-06-09 00:47:50,771 - Training step 1140/2000 |**************           | - loss: 1313.2537 - ci: 0.7170\n",
      "2017-06-09 00:47:52,541 - Training step 1150/2000 |**************           | - loss: 1303.2732 - ci: 0.7168\n",
      "2017-06-09 00:47:52,541 - Training step 1150/2000 |**************           | - loss: 1303.2732 - ci: 0.7168\n",
      "2017-06-09 00:47:52,541 - Training step 1150/2000 |**************           | - loss: 1303.2732 - ci: 0.7168\n",
      "2017-06-09 00:47:54,112 - Training step 1160/2000 |**************           | - loss: 1308.7607 - ci: 0.7165\n",
      "2017-06-09 00:47:54,112 - Training step 1160/2000 |**************           | - loss: 1308.7607 - ci: 0.7165\n",
      "2017-06-09 00:47:54,112 - Training step 1160/2000 |**************           | - loss: 1308.7607 - ci: 0.7165\n",
      "2017-06-09 00:47:55,599 - Training step 1170/2000 |**************           | - loss: 1312.3324 - ci: 0.7168\n",
      "2017-06-09 00:47:55,599 - Training step 1170/2000 |**************           | - loss: 1312.3324 - ci: 0.7168\n",
      "2017-06-09 00:47:55,599 - Training step 1170/2000 |**************           | - loss: 1312.3324 - ci: 0.7168\n",
      "2017-06-09 00:47:57,353 - Training step 1180/2000 |**************           | - loss: 1311.0442 - ci: 0.7168\n",
      "2017-06-09 00:47:57,353 - Training step 1180/2000 |**************           | - loss: 1311.0442 - ci: 0.7168\n",
      "2017-06-09 00:47:57,353 - Training step 1180/2000 |**************           | - loss: 1311.0442 - ci: 0.7168\n",
      "2017-06-09 00:47:58,958 - Training step 1190/2000 |**************           | - loss: 1307.1786 - ci: 0.7166\n",
      "2017-06-09 00:47:58,958 - Training step 1190/2000 |**************           | - loss: 1307.1786 - ci: 0.7166\n",
      "2017-06-09 00:47:58,958 - Training step 1190/2000 |**************           | - loss: 1307.1786 - ci: 0.7166\n",
      "2017-06-09 00:48:00,617 - Training step 1200/2000 |***************          | - loss: 1306.9219 - ci: 0.7166\n",
      "2017-06-09 00:48:00,617 - Training step 1200/2000 |***************          | - loss: 1306.9219 - ci: 0.7166\n",
      "2017-06-09 00:48:00,617 - Training step 1200/2000 |***************          | - loss: 1306.9219 - ci: 0.7166\n",
      "2017-06-09 00:48:02,028 - Training step 1210/2000 |***************          | - loss: 1311.4526 - ci: 0.7168\n",
      "2017-06-09 00:48:02,028 - Training step 1210/2000 |***************          | - loss: 1311.4526 - ci: 0.7168\n",
      "2017-06-09 00:48:02,028 - Training step 1210/2000 |***************          | - loss: 1311.4526 - ci: 0.7168\n",
      "2017-06-09 00:48:03,632 - Training step 1220/2000 |***************          | - loss: 1317.0579 - ci: 0.7170\n",
      "2017-06-09 00:48:03,632 - Training step 1220/2000 |***************          | - loss: 1317.0579 - ci: 0.7170\n",
      "2017-06-09 00:48:03,632 - Training step 1220/2000 |***************          | - loss: 1317.0579 - ci: 0.7170\n",
      "2017-06-09 00:48:05,198 - Training step 1230/2000 |***************          | - loss: 1308.2753 - ci: 0.7166\n",
      "2017-06-09 00:48:05,198 - Training step 1230/2000 |***************          | - loss: 1308.2753 - ci: 0.7166\n",
      "2017-06-09 00:48:05,198 - Training step 1230/2000 |***************          | - loss: 1308.2753 - ci: 0.7166\n",
      "2017-06-09 00:48:06,607 - Training step 1240/2000 |***************          | - loss: 1302.5891 - ci: 0.7170\n",
      "2017-06-09 00:48:06,607 - Training step 1240/2000 |***************          | - loss: 1302.5891 - ci: 0.7170\n",
      "2017-06-09 00:48:06,607 - Training step 1240/2000 |***************          | - loss: 1302.5891 - ci: 0.7170\n",
      "2017-06-09 00:48:08,149 - Training step 1250/2000 |***************          | - loss: 1305.9572 - ci: 0.7171\n",
      "2017-06-09 00:48:08,149 - Training step 1250/2000 |***************          | - loss: 1305.9572 - ci: 0.7171\n",
      "2017-06-09 00:48:08,149 - Training step 1250/2000 |***************          | - loss: 1305.9572 - ci: 0.7171\n",
      "2017-06-09 00:48:09,700 - Training step 1260/2000 |***************          | - loss: 1313.5267 - ci: 0.7170\n",
      "2017-06-09 00:48:09,700 - Training step 1260/2000 |***************          | - loss: 1313.5267 - ci: 0.7170\n",
      "2017-06-09 00:48:09,700 - Training step 1260/2000 |***************          | - loss: 1313.5267 - ci: 0.7170\n",
      "2017-06-09 00:48:11,097 - Training step 1270/2000 |***************          | - loss: 1314.9220 - ci: 0.7171\n",
      "2017-06-09 00:48:11,097 - Training step 1270/2000 |***************          | - loss: 1314.9220 - ci: 0.7171\n",
      "2017-06-09 00:48:11,097 - Training step 1270/2000 |***************          | - loss: 1314.9220 - ci: 0.7171\n",
      "2017-06-09 00:48:13,150 - Training step 1280/2000 |****************         | - loss: 1304.7164 - ci: 0.7172\n",
      "2017-06-09 00:48:13,150 - Training step 1280/2000 |****************         | - loss: 1304.7164 - ci: 0.7172\n",
      "2017-06-09 00:48:13,150 - Training step 1280/2000 |****************         | - loss: 1304.7164 - ci: 0.7172\n",
      "2017-06-09 00:48:14,942 - Training step 1290/2000 |****************         | - loss: 1305.4498 - ci: 0.7172\n",
      "2017-06-09 00:48:14,942 - Training step 1290/2000 |****************         | - loss: 1305.4498 - ci: 0.7172\n",
      "2017-06-09 00:48:14,942 - Training step 1290/2000 |****************         | - loss: 1305.4498 - ci: 0.7172\n",
      "2017-06-09 00:48:16,413 - Training step 1300/2000 |****************         | - loss: 1312.6072 - ci: 0.7170\n",
      "2017-06-09 00:48:16,413 - Training step 1300/2000 |****************         | - loss: 1312.6072 - ci: 0.7170\n",
      "2017-06-09 00:48:16,413 - Training step 1300/2000 |****************         | - loss: 1312.6072 - ci: 0.7170\n",
      "2017-06-09 00:48:17,969 - Training step 1310/2000 |****************         | - loss: 1318.5777 - ci: 0.7166\n",
      "2017-06-09 00:48:17,969 - Training step 1310/2000 |****************         | - loss: 1318.5777 - ci: 0.7166\n",
      "2017-06-09 00:48:17,969 - Training step 1310/2000 |****************         | - loss: 1318.5777 - ci: 0.7166\n",
      "2017-06-09 00:48:19,884 - Training step 1320/2000 |****************         | - loss: 1299.2790 - ci: 0.7164\n",
      "2017-06-09 00:48:19,884 - Training step 1320/2000 |****************         | - loss: 1299.2790 - ci: 0.7164\n",
      "2017-06-09 00:48:19,884 - Training step 1320/2000 |****************         | - loss: 1299.2790 - ci: 0.7164\n",
      "2017-06-09 00:48:21,745 - Training step 1330/2000 |****************         | - loss: 1305.1444 - ci: 0.7164\n",
      "2017-06-09 00:48:21,745 - Training step 1330/2000 |****************         | - loss: 1305.1444 - ci: 0.7164\n",
      "2017-06-09 00:48:21,745 - Training step 1330/2000 |****************         | - loss: 1305.1444 - ci: 0.7164\n",
      "2017-06-09 00:48:23,384 - Training step 1340/2000 |****************         | - loss: 1307.1364 - ci: 0.7168\n",
      "2017-06-09 00:48:23,384 - Training step 1340/2000 |****************         | - loss: 1307.1364 - ci: 0.7168\n",
      "2017-06-09 00:48:23,384 - Training step 1340/2000 |****************         | - loss: 1307.1364 - ci: 0.7168\n",
      "2017-06-09 00:48:24,919 - Training step 1350/2000 |****************         | - loss: 1304.9625 - ci: 0.7166\n",
      "2017-06-09 00:48:24,919 - Training step 1350/2000 |****************         | - loss: 1304.9625 - ci: 0.7166\n",
      "2017-06-09 00:48:24,919 - Training step 1350/2000 |****************         | - loss: 1304.9625 - ci: 0.7166\n",
      "2017-06-09 00:48:26,539 - Training step 1360/2000 |*****************        | - loss: 1314.5679 - ci: 0.7165\n",
      "2017-06-09 00:48:26,539 - Training step 1360/2000 |*****************        | - loss: 1314.5679 - ci: 0.7165\n",
      "2017-06-09 00:48:26,539 - Training step 1360/2000 |*****************        | - loss: 1314.5679 - ci: 0.7165\n",
      "2017-06-09 00:48:27,950 - Training step 1370/2000 |*****************        | - loss: 1301.6400 - ci: 0.7165\n",
      "2017-06-09 00:48:27,950 - Training step 1370/2000 |*****************        | - loss: 1301.6400 - ci: 0.7165\n",
      "2017-06-09 00:48:27,950 - Training step 1370/2000 |*****************        | - loss: 1301.6400 - ci: 0.7165\n",
      "2017-06-09 00:48:29,480 - Training step 1380/2000 |*****************        | - loss: 1305.9058 - ci: 0.7164\n",
      "2017-06-09 00:48:29,480 - Training step 1380/2000 |*****************        | - loss: 1305.9058 - ci: 0.7164\n",
      "2017-06-09 00:48:29,480 - Training step 1380/2000 |*****************        | - loss: 1305.9058 - ci: 0.7164\n",
      "2017-06-09 00:48:31,083 - Training step 1390/2000 |*****************        | - loss: 1313.5641 - ci: 0.7172\n",
      "2017-06-09 00:48:31,083 - Training step 1390/2000 |*****************        | - loss: 1313.5641 - ci: 0.7172\n",
      "2017-06-09 00:48:31,083 - Training step 1390/2000 |*****************        | - loss: 1313.5641 - ci: 0.7172\n",
      "2017-06-09 00:48:32,781 - Training step 1400/2000 |*****************        | - loss: 1303.2757 - ci: 0.7165\n",
      "2017-06-09 00:48:32,781 - Training step 1400/2000 |*****************        | - loss: 1303.2757 - ci: 0.7165\n",
      "2017-06-09 00:48:32,781 - Training step 1400/2000 |*****************        | - loss: 1303.2757 - ci: 0.7165\n",
      "2017-06-09 00:48:34,510 - Training step 1410/2000 |*****************        | - loss: 1307.2826 - ci: 0.7169\n",
      "2017-06-09 00:48:34,510 - Training step 1410/2000 |*****************        | - loss: 1307.2826 - ci: 0.7169\n",
      "2017-06-09 00:48:34,510 - Training step 1410/2000 |*****************        | - loss: 1307.2826 - ci: 0.7169\n",
      "2017-06-09 00:48:36,606 - Training step 1420/2000 |*****************        | - loss: 1309.8158 - ci: 0.7171\n",
      "2017-06-09 00:48:36,606 - Training step 1420/2000 |*****************        | - loss: 1309.8158 - ci: 0.7171\n",
      "2017-06-09 00:48:36,606 - Training step 1420/2000 |*****************        | - loss: 1309.8158 - ci: 0.7171\n",
      "2017-06-09 00:48:38,431 - Training step 1430/2000 |*****************        | - loss: 1302.8519 - ci: 0.7168\n",
      "2017-06-09 00:48:38,431 - Training step 1430/2000 |*****************        | - loss: 1302.8519 - ci: 0.7168\n",
      "2017-06-09 00:48:38,431 - Training step 1430/2000 |*****************        | - loss: 1302.8519 - ci: 0.7168\n",
      "2017-06-09 00:48:40,206 - Training step 1440/2000 |******************       | - loss: 1306.5000 - ci: 0.7167\n",
      "2017-06-09 00:48:40,206 - Training step 1440/2000 |******************       | - loss: 1306.5000 - ci: 0.7167\n",
      "2017-06-09 00:48:40,206 - Training step 1440/2000 |******************       | - loss: 1306.5000 - ci: 0.7167\n",
      "2017-06-09 00:48:41,775 - Training step 1450/2000 |******************       | - loss: 1305.4064 - ci: 0.7171\n",
      "2017-06-09 00:48:41,775 - Training step 1450/2000 |******************       | - loss: 1305.4064 - ci: 0.7171\n",
      "2017-06-09 00:48:41,775 - Training step 1450/2000 |******************       | - loss: 1305.4064 - ci: 0.7171\n",
      "2017-06-09 00:48:43,271 - Training step 1460/2000 |******************       | - loss: 1301.8946 - ci: 0.7166\n",
      "2017-06-09 00:48:43,271 - Training step 1460/2000 |******************       | - loss: 1301.8946 - ci: 0.7166\n",
      "2017-06-09 00:48:43,271 - Training step 1460/2000 |******************       | - loss: 1301.8946 - ci: 0.7166\n",
      "2017-06-09 00:48:44,792 - Training step 1470/2000 |******************       | - loss: 1305.7367 - ci: 0.7164\n",
      "2017-06-09 00:48:44,792 - Training step 1470/2000 |******************       | - loss: 1305.7367 - ci: 0.7164\n",
      "2017-06-09 00:48:44,792 - Training step 1470/2000 |******************       | - loss: 1305.7367 - ci: 0.7164\n",
      "2017-06-09 00:48:46,353 - Training step 1480/2000 |******************       | - loss: 1305.8437 - ci: 0.7167\n",
      "2017-06-09 00:48:46,353 - Training step 1480/2000 |******************       | - loss: 1305.8437 - ci: 0.7167\n",
      "2017-06-09 00:48:46,353 - Training step 1480/2000 |******************       | - loss: 1305.8437 - ci: 0.7167\n",
      "2017-06-09 00:48:48,201 - Training step 1490/2000 |******************       | - loss: 1310.8111 - ci: 0.7165\n",
      "2017-06-09 00:48:48,201 - Training step 1490/2000 |******************       | - loss: 1310.8111 - ci: 0.7165\n",
      "2017-06-09 00:48:48,201 - Training step 1490/2000 |******************       | - loss: 1310.8111 - ci: 0.7165\n",
      "2017-06-09 00:48:49,828 - Training step 1500/2000 |******************       | - loss: 1307.7474 - ci: 0.7174\n",
      "2017-06-09 00:48:49,828 - Training step 1500/2000 |******************       | - loss: 1307.7474 - ci: 0.7174\n",
      "2017-06-09 00:48:49,828 - Training step 1500/2000 |******************       | - loss: 1307.7474 - ci: 0.7174\n",
      "2017-06-09 00:48:51,768 - Training step 1510/2000 |******************       | - loss: 1309.9571 - ci: 0.7168\n",
      "2017-06-09 00:48:51,768 - Training step 1510/2000 |******************       | - loss: 1309.9571 - ci: 0.7168\n",
      "2017-06-09 00:48:51,768 - Training step 1510/2000 |******************       | - loss: 1309.9571 - ci: 0.7168\n",
      "2017-06-09 00:48:53,611 - Training step 1520/2000 |*******************      | - loss: 1301.3815 - ci: 0.7167\n",
      "2017-06-09 00:48:53,611 - Training step 1520/2000 |*******************      | - loss: 1301.3815 - ci: 0.7167\n",
      "2017-06-09 00:48:53,611 - Training step 1520/2000 |*******************      | - loss: 1301.3815 - ci: 0.7167\n",
      "2017-06-09 00:48:55,207 - Training step 1530/2000 |*******************      | - loss: 1302.8541 - ci: 0.7171\n",
      "2017-06-09 00:48:55,207 - Training step 1530/2000 |*******************      | - loss: 1302.8541 - ci: 0.7171\n",
      "2017-06-09 00:48:55,207 - Training step 1530/2000 |*******************      | - loss: 1302.8541 - ci: 0.7171\n",
      "2017-06-09 00:48:56,637 - Training step 1540/2000 |*******************      | - loss: 1307.4468 - ci: 0.7169\n",
      "2017-06-09 00:48:56,637 - Training step 1540/2000 |*******************      | - loss: 1307.4468 - ci: 0.7169\n",
      "2017-06-09 00:48:56,637 - Training step 1540/2000 |*******************      | - loss: 1307.4468 - ci: 0.7169\n",
      "2017-06-09 00:48:58,191 - Training step 1550/2000 |*******************      | - loss: 1299.4765 - ci: 0.7171\n",
      "2017-06-09 00:48:58,191 - Training step 1550/2000 |*******************      | - loss: 1299.4765 - ci: 0.7171\n",
      "2017-06-09 00:48:58,191 - Training step 1550/2000 |*******************      | - loss: 1299.4765 - ci: 0.7171\n",
      "2017-06-09 00:48:59,825 - Training step 1560/2000 |*******************      | - loss: 1306.8862 - ci: 0.7170\n",
      "2017-06-09 00:48:59,825 - Training step 1560/2000 |*******************      | - loss: 1306.8862 - ci: 0.7170\n",
      "2017-06-09 00:48:59,825 - Training step 1560/2000 |*******************      | - loss: 1306.8862 - ci: 0.7170\n",
      "2017-06-09 00:49:01,263 - Training step 1570/2000 |*******************      | - loss: 1304.6097 - ci: 0.7171\n",
      "2017-06-09 00:49:01,263 - Training step 1570/2000 |*******************      | - loss: 1304.6097 - ci: 0.7171\n",
      "2017-06-09 00:49:01,263 - Training step 1570/2000 |*******************      | - loss: 1304.6097 - ci: 0.7171\n",
      "2017-06-09 00:49:02,839 - Training step 1580/2000 |*******************      | - loss: 1304.6801 - ci: 0.7174\n",
      "2017-06-09 00:49:02,839 - Training step 1580/2000 |*******************      | - loss: 1304.6801 - ci: 0.7174\n",
      "2017-06-09 00:49:02,839 - Training step 1580/2000 |*******************      | - loss: 1304.6801 - ci: 0.7174\n",
      "2017-06-09 00:49:04,410 - Training step 1590/2000 |*******************      | - loss: 1315.5168 - ci: 0.7171\n",
      "2017-06-09 00:49:04,410 - Training step 1590/2000 |*******************      | - loss: 1315.5168 - ci: 0.7171\n",
      "2017-06-09 00:49:04,410 - Training step 1590/2000 |*******************      | - loss: 1315.5168 - ci: 0.7171\n",
      "2017-06-09 00:49:06,072 - Training step 1600/2000 |********************     | - loss: 1301.2437 - ci: 0.7172\n",
      "2017-06-09 00:49:06,072 - Training step 1600/2000 |********************     | - loss: 1301.2437 - ci: 0.7172\n",
      "2017-06-09 00:49:06,072 - Training step 1600/2000 |********************     | - loss: 1301.2437 - ci: 0.7172\n",
      "2017-06-09 00:49:07,450 - Training step 1610/2000 |********************     | - loss: 1309.3214 - ci: 0.7173\n",
      "2017-06-09 00:49:07,450 - Training step 1610/2000 |********************     | - loss: 1309.3214 - ci: 0.7173\n",
      "2017-06-09 00:49:07,450 - Training step 1610/2000 |********************     | - loss: 1309.3214 - ci: 0.7173\n",
      "2017-06-09 00:49:08,995 - Training step 1620/2000 |********************     | - loss: 1308.7008 - ci: 0.7171\n",
      "2017-06-09 00:49:08,995 - Training step 1620/2000 |********************     | - loss: 1308.7008 - ci: 0.7171\n",
      "2017-06-09 00:49:08,995 - Training step 1620/2000 |********************     | - loss: 1308.7008 - ci: 0.7171\n",
      "2017-06-09 00:49:10,646 - Training step 1630/2000 |********************     | - loss: 1307.7787 - ci: 0.7174\n",
      "2017-06-09 00:49:10,646 - Training step 1630/2000 |********************     | - loss: 1307.7787 - ci: 0.7174\n",
      "2017-06-09 00:49:10,646 - Training step 1630/2000 |********************     | - loss: 1307.7787 - ci: 0.7174\n",
      "2017-06-09 00:49:12,189 - Training step 1640/2000 |********************     | - loss: 1308.7579 - ci: 0.7171\n",
      "2017-06-09 00:49:12,189 - Training step 1640/2000 |********************     | - loss: 1308.7579 - ci: 0.7171\n",
      "2017-06-09 00:49:12,189 - Training step 1640/2000 |********************     | - loss: 1308.7579 - ci: 0.7171\n",
      "2017-06-09 00:49:13,770 - Training step 1650/2000 |********************     | - loss: 1306.4091 - ci: 0.7175\n",
      "2017-06-09 00:49:13,770 - Training step 1650/2000 |********************     | - loss: 1306.4091 - ci: 0.7175\n",
      "2017-06-09 00:49:13,770 - Training step 1650/2000 |********************     | - loss: 1306.4091 - ci: 0.7175\n",
      "2017-06-09 00:49:15,446 - Training step 1660/2000 |********************     | - loss: 1307.1218 - ci: 0.7172\n",
      "2017-06-09 00:49:15,446 - Training step 1660/2000 |********************     | - loss: 1307.1218 - ci: 0.7172\n",
      "2017-06-09 00:49:15,446 - Training step 1660/2000 |********************     | - loss: 1307.1218 - ci: 0.7172\n",
      "2017-06-09 00:49:17,031 - Training step 1670/2000 |********************     | - loss: 1306.0028 - ci: 0.7173\n",
      "2017-06-09 00:49:17,031 - Training step 1670/2000 |********************     | - loss: 1306.0028 - ci: 0.7173\n",
      "2017-06-09 00:49:17,031 - Training step 1670/2000 |********************     | - loss: 1306.0028 - ci: 0.7173\n",
      "2017-06-09 00:49:18,601 - Training step 1680/2000 |*********************    | - loss: 1303.7675 - ci: 0.7173\n",
      "2017-06-09 00:49:18,601 - Training step 1680/2000 |*********************    | - loss: 1303.7675 - ci: 0.7173\n",
      "2017-06-09 00:49:18,601 - Training step 1680/2000 |*********************    | - loss: 1303.7675 - ci: 0.7173\n",
      "2017-06-09 00:49:20,115 - Training step 1690/2000 |*********************    | - loss: 1298.7003 - ci: 0.7173\n",
      "2017-06-09 00:49:20,115 - Training step 1690/2000 |*********************    | - loss: 1298.7003 - ci: 0.7173\n",
      "2017-06-09 00:49:20,115 - Training step 1690/2000 |*********************    | - loss: 1298.7003 - ci: 0.7173\n",
      "2017-06-09 00:49:21,697 - Training step 1700/2000 |*********************    | - loss: 1301.8650 - ci: 0.7174\n",
      "2017-06-09 00:49:21,697 - Training step 1700/2000 |*********************    | - loss: 1301.8650 - ci: 0.7174\n",
      "2017-06-09 00:49:21,697 - Training step 1700/2000 |*********************    | - loss: 1301.8650 - ci: 0.7174\n",
      "2017-06-09 00:49:23,154 - Training step 1710/2000 |*********************    | - loss: 1308.9804 - ci: 0.7174\n",
      "2017-06-09 00:49:23,154 - Training step 1710/2000 |*********************    | - loss: 1308.9804 - ci: 0.7174\n",
      "2017-06-09 00:49:23,154 - Training step 1710/2000 |*********************    | - loss: 1308.9804 - ci: 0.7174\n",
      "2017-06-09 00:49:24,903 - Training step 1720/2000 |*********************    | - loss: 1309.4044 - ci: 0.7176\n",
      "2017-06-09 00:49:24,903 - Training step 1720/2000 |*********************    | - loss: 1309.4044 - ci: 0.7176\n",
      "2017-06-09 00:49:24,903 - Training step 1720/2000 |*********************    | - loss: 1309.4044 - ci: 0.7176\n",
      "2017-06-09 00:49:26,473 - Training step 1730/2000 |*********************    | - loss: 1314.2817 - ci: 0.7175\n",
      "2017-06-09 00:49:26,473 - Training step 1730/2000 |*********************    | - loss: 1314.2817 - ci: 0.7175\n",
      "2017-06-09 00:49:26,473 - Training step 1730/2000 |*********************    | - loss: 1314.2817 - ci: 0.7175\n",
      "2017-06-09 00:49:27,921 - Training step 1740/2000 |*********************    | - loss: 1309.6833 - ci: 0.7178\n",
      "2017-06-09 00:49:27,921 - Training step 1740/2000 |*********************    | - loss: 1309.6833 - ci: 0.7178\n",
      "2017-06-09 00:49:27,921 - Training step 1740/2000 |*********************    | - loss: 1309.6833 - ci: 0.7178\n",
      "2017-06-09 00:49:29,498 - Training step 1750/2000 |*********************    | - loss: 1298.7552 - ci: 0.7172\n",
      "2017-06-09 00:49:29,498 - Training step 1750/2000 |*********************    | - loss: 1298.7552 - ci: 0.7172\n",
      "2017-06-09 00:49:29,498 - Training step 1750/2000 |*********************    | - loss: 1298.7552 - ci: 0.7172\n",
      "2017-06-09 00:49:30,982 - Training step 1760/2000 |**********************   | - loss: 1306.7221 - ci: 0.7171\n",
      "2017-06-09 00:49:30,982 - Training step 1760/2000 |**********************   | - loss: 1306.7221 - ci: 0.7171\n",
      "2017-06-09 00:49:30,982 - Training step 1760/2000 |**********************   | - loss: 1306.7221 - ci: 0.7171\n",
      "2017-06-09 00:49:32,477 - Training step 1770/2000 |**********************   | - loss: 1303.1513 - ci: 0.7173\n",
      "2017-06-09 00:49:32,477 - Training step 1770/2000 |**********************   | - loss: 1303.1513 - ci: 0.7173\n",
      "2017-06-09 00:49:32,477 - Training step 1770/2000 |**********************   | - loss: 1303.1513 - ci: 0.7173\n",
      "2017-06-09 00:49:33,820 - Training step 1780/2000 |**********************   | - loss: 1306.6945 - ci: 0.7175\n",
      "2017-06-09 00:49:33,820 - Training step 1780/2000 |**********************   | - loss: 1306.6945 - ci: 0.7175\n",
      "2017-06-09 00:49:33,820 - Training step 1780/2000 |**********************   | - loss: 1306.6945 - ci: 0.7175\n",
      "2017-06-09 00:49:35,320 - Training step 1790/2000 |**********************   | - loss: 1306.0719 - ci: 0.7182\n",
      "2017-06-09 00:49:35,320 - Training step 1790/2000 |**********************   | - loss: 1306.0719 - ci: 0.7182\n",
      "2017-06-09 00:49:35,320 - Training step 1790/2000 |**********************   | - loss: 1306.0719 - ci: 0.7182\n",
      "2017-06-09 00:49:36,922 - Training step 1800/2000 |**********************   | - loss: 1311.2468 - ci: 0.7175\n",
      "2017-06-09 00:49:36,922 - Training step 1800/2000 |**********************   | - loss: 1311.2468 - ci: 0.7175\n",
      "2017-06-09 00:49:36,922 - Training step 1800/2000 |**********************   | - loss: 1311.2468 - ci: 0.7175\n",
      "2017-06-09 00:49:38,366 - Training step 1810/2000 |**********************   | - loss: 1295.1567 - ci: 0.7181\n",
      "2017-06-09 00:49:38,366 - Training step 1810/2000 |**********************   | - loss: 1295.1567 - ci: 0.7181\n",
      "2017-06-09 00:49:38,366 - Training step 1810/2000 |**********************   | - loss: 1295.1567 - ci: 0.7181\n",
      "2017-06-09 00:49:39,810 - Training step 1820/2000 |**********************   | - loss: 1302.8696 - ci: 0.7175\n",
      "2017-06-09 00:49:39,810 - Training step 1820/2000 |**********************   | - loss: 1302.8696 - ci: 0.7175\n",
      "2017-06-09 00:49:39,810 - Training step 1820/2000 |**********************   | - loss: 1302.8696 - ci: 0.7175\n",
      "2017-06-09 00:49:41,212 - Training step 1830/2000 |**********************   | - loss: 1295.8949 - ci: 0.7174\n",
      "2017-06-09 00:49:41,212 - Training step 1830/2000 |**********************   | - loss: 1295.8949 - ci: 0.7174\n",
      "2017-06-09 00:49:41,212 - Training step 1830/2000 |**********************   | - loss: 1295.8949 - ci: 0.7174\n",
      "2017-06-09 00:49:42,525 - Training step 1840/2000 |***********************  | - loss: 1306.5958 - ci: 0.7175\n",
      "2017-06-09 00:49:42,525 - Training step 1840/2000 |***********************  | - loss: 1306.5958 - ci: 0.7175\n",
      "2017-06-09 00:49:42,525 - Training step 1840/2000 |***********************  | - loss: 1306.5958 - ci: 0.7175\n",
      "2017-06-09 00:49:43,972 - Training step 1850/2000 |***********************  | - loss: 1309.5176 - ci: 0.7177\n",
      "2017-06-09 00:49:43,972 - Training step 1850/2000 |***********************  | - loss: 1309.5176 - ci: 0.7177\n",
      "2017-06-09 00:49:43,972 - Training step 1850/2000 |***********************  | - loss: 1309.5176 - ci: 0.7177\n",
      "2017-06-09 00:49:45,440 - Training step 1860/2000 |***********************  | - loss: 1310.6745 - ci: 0.7176\n",
      "2017-06-09 00:49:45,440 - Training step 1860/2000 |***********************  | - loss: 1310.6745 - ci: 0.7176\n",
      "2017-06-09 00:49:45,440 - Training step 1860/2000 |***********************  | - loss: 1310.6745 - ci: 0.7176\n",
      "2017-06-09 00:49:46,839 - Training step 1870/2000 |***********************  | - loss: 1308.0200 - ci: 0.7178\n",
      "2017-06-09 00:49:46,839 - Training step 1870/2000 |***********************  | - loss: 1308.0200 - ci: 0.7178\n",
      "2017-06-09 00:49:46,839 - Training step 1870/2000 |***********************  | - loss: 1308.0200 - ci: 0.7178\n",
      "2017-06-09 00:49:48,448 - Training step 1880/2000 |***********************  | - loss: 1307.3695 - ci: 0.7170\n",
      "2017-06-09 00:49:48,448 - Training step 1880/2000 |***********************  | - loss: 1307.3695 - ci: 0.7170\n",
      "2017-06-09 00:49:48,448 - Training step 1880/2000 |***********************  | - loss: 1307.3695 - ci: 0.7170\n",
      "2017-06-09 00:49:50,024 - Training step 1890/2000 |***********************  | - loss: 1304.3683 - ci: 0.7175\n",
      "2017-06-09 00:49:50,024 - Training step 1890/2000 |***********************  | - loss: 1304.3683 - ci: 0.7175\n",
      "2017-06-09 00:49:50,024 - Training step 1890/2000 |***********************  | - loss: 1304.3683 - ci: 0.7175\n",
      "2017-06-09 00:49:51,505 - Training step 1900/2000 |***********************  | - loss: 1301.3851 - ci: 0.7171\n",
      "2017-06-09 00:49:51,505 - Training step 1900/2000 |***********************  | - loss: 1301.3851 - ci: 0.7171\n",
      "2017-06-09 00:49:51,505 - Training step 1900/2000 |***********************  | - loss: 1301.3851 - ci: 0.7171\n",
      "2017-06-09 00:49:53,285 - Training step 1910/2000 |***********************  | - loss: 1302.7624 - ci: 0.7171\n",
      "2017-06-09 00:49:53,285 - Training step 1910/2000 |***********************  | - loss: 1302.7624 - ci: 0.7171\n",
      "2017-06-09 00:49:53,285 - Training step 1910/2000 |***********************  | - loss: 1302.7624 - ci: 0.7171\n",
      "2017-06-09 00:49:55,362 - Training step 1920/2000 |************************ | - loss: 1312.6177 - ci: 0.7172\n",
      "2017-06-09 00:49:55,362 - Training step 1920/2000 |************************ | - loss: 1312.6177 - ci: 0.7172\n",
      "2017-06-09 00:49:55,362 - Training step 1920/2000 |************************ | - loss: 1312.6177 - ci: 0.7172\n",
      "2017-06-09 00:49:57,242 - Training step 1930/2000 |************************ | - loss: 1306.5119 - ci: 0.7171\n",
      "2017-06-09 00:49:57,242 - Training step 1930/2000 |************************ | - loss: 1306.5119 - ci: 0.7171\n",
      "2017-06-09 00:49:57,242 - Training step 1930/2000 |************************ | - loss: 1306.5119 - ci: 0.7171\n",
      "2017-06-09 00:49:58,764 - Training step 1940/2000 |************************ | - loss: 1308.2971 - ci: 0.7171\n",
      "2017-06-09 00:49:58,764 - Training step 1940/2000 |************************ | - loss: 1308.2971 - ci: 0.7171\n",
      "2017-06-09 00:49:58,764 - Training step 1940/2000 |************************ | - loss: 1308.2971 - ci: 0.7171\n",
      "2017-06-09 00:50:00,450 - Training step 1950/2000 |************************ | - loss: 1302.4246 - ci: 0.7174\n",
      "2017-06-09 00:50:00,450 - Training step 1950/2000 |************************ | - loss: 1302.4246 - ci: 0.7174\n",
      "2017-06-09 00:50:00,450 - Training step 1950/2000 |************************ | - loss: 1302.4246 - ci: 0.7174\n",
      "2017-06-09 00:50:02,139 - Training step 1960/2000 |************************ | - loss: 1315.1864 - ci: 0.7175\n",
      "2017-06-09 00:50:02,139 - Training step 1960/2000 |************************ | - loss: 1315.1864 - ci: 0.7175\n",
      "2017-06-09 00:50:02,139 - Training step 1960/2000 |************************ | - loss: 1315.1864 - ci: 0.7175\n",
      "2017-06-09 00:50:03,562 - Training step 1970/2000 |************************ | - loss: 1312.8696 - ci: 0.7174\n",
      "2017-06-09 00:50:03,562 - Training step 1970/2000 |************************ | - loss: 1312.8696 - ci: 0.7174\n",
      "2017-06-09 00:50:03,562 - Training step 1970/2000 |************************ | - loss: 1312.8696 - ci: 0.7174\n",
      "2017-06-09 00:50:05,144 - Training step 1980/2000 |************************ | - loss: 1304.4548 - ci: 0.7171\n",
      "2017-06-09 00:50:05,144 - Training step 1980/2000 |************************ | - loss: 1304.4548 - ci: 0.7171\n",
      "2017-06-09 00:50:05,144 - Training step 1980/2000 |************************ | - loss: 1304.4548 - ci: 0.7171\n",
      "2017-06-09 00:50:06,791 - Training step 1990/2000 |************************ | - loss: 1306.8314 - ci: 0.7178\n",
      "2017-06-09 00:50:06,791 - Training step 1990/2000 |************************ | - loss: 1306.8314 - ci: 0.7178\n",
      "2017-06-09 00:50:06,791 - Training step 1990/2000 |************************ | - loss: 1306.8314 - ci: 0.7178\n",
      "2017-06-09 00:50:08,133 - Finished Training with 2000 iterations in 313.59s\n",
      "2017-06-09 00:50:08,133 - Finished Training with 2000 iterations in 313.59s\n",
      "2017-06-09 00:50:08,133 - Finished Training with 2000 iterations in 313.59s\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of DeepSurv using the hyperparams defined above\n",
    "model = deep_surv.DeepSurv(**hyperparams)\n",
    "\n",
    "# DeepSurv can now leverage TensorBoard to monitor training and validation\n",
    "# This section of code is optional. If you don't want to use the tensorboard logger\n",
    "# Uncomment the below line, and comment out the other three lines: \n",
    "# logger = None\n",
    "\n",
    "experiment_name = 'test_experiment_sebastian'\n",
    "logdir = './logs/tensorboard/'\n",
    "logger = TensorboardLogger(experiment_name, logdir=logdir)\n",
    "\n",
    "# Now we train the model\n",
    "update_fn=lasagne.updates.nesterov_momentum # The type of optimizer to use. \\\n",
    "                                            # Check out http://lasagne.readthedocs.io/en/latest/modules/updates.html \\\n",
    "                                            # for other optimizers to use\n",
    "n_epochs = 2000\n",
    "\n",
    "# If you have validation data, you can add it as the second parameter to the function\n",
    "metrics = model.train(train_data, n_epochs=n_epochs, logger=logger, update_fn=update_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two different ways to visualzie how the model trained:\n",
    "\n",
    "- Tensorboard (install ()[tensorboard]) which provides realtime metrics. Run the command in shell:\n",
    "   \n",
    "   `tensorboard --logdir './logs/tensorboard'`\n",
    "     \n",
    "     \n",
    "- Visualize the training functions post training (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train C-Index: (1999, 0.71723313860860327)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f2413c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZtYvTiRmZtYvTiRmZtYv/x+htjmaZg/CJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ed1a860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the final metrics\n",
    "print('Train C-Index:', metrics['c-index'][-1])\n",
    "# print('Valid C-Index: ',metrics['valid_c-index'][-1])\n",
    "\n",
    "# Plot the training / validation curves\n",
    "viz.plot_log(metrics)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
